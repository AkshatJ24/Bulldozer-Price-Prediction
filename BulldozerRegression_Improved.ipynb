{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-20T19:43:26.392655Z",
     "start_time": "2025-08-20T19:43:26.380874Z"
    }
   },
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Listing the data available\n",
    "import os\n",
    "folder = 'bluebook-for-bulldozers'\n",
    "for item in os.listdir(folder):\n",
    "    print(item)\n",
    "\n",
    "# Loading TRAINING data separately (NOT combined TrainAndValid.csv)\n",
    "df_train = pd.read_csv(\"bluebook-for-bulldozers/Train.csv\", low_memory=False, parse_dates=['saledate'])\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "df_train.head(10)\n",
    "\n",
    "# Loading VALIDATION data separately and merging with validation solutions\n",
    "df_valid_features = pd.read_csv(\"bluebook-for-bulldozers/Valid.csv\", low_memory=False, parse_dates=['saledate'])\n",
    "df_valid_targets = pd.read_csv(\"bluebook-for-bulldozers/ValidSolution.csv\")\n",
    "\n",
    "# Merge validation features with targets\n",
    "df_valid = df_valid_features.merge(df_valid_targets[['SalesID', 'SalePrice']], on='SalesID', how='left')\n",
    "print(f\"Validation data shape: {df_valid.shape}\")\n",
    "print(f\"Validation features shape: {df_valid_features.shape}\")\n",
    "print(f\"Validation targets shape: {df_valid_targets.shape}\")\n",
    "df_valid.head(10)\n",
    "\n",
    "# Loading TEST data\n",
    "df_test = pd.read_csv(\"bluebook-for-bulldozers/Test.csv\", low_memory=False, parse_dates=['saledate'])\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "df_test.head(10)\n",
    "\n",
    "# Checking the date ranges to confirm proper separation\n",
    "print(\"Training data date range:\")\n",
    "print(f\"From: {df_train['saledate'].min()} To: {df_train['saledate'].max()}\")\n",
    "print(\"\\nValidation data date range:\")\n",
    "print(f\"From: {df_valid['saledate'].min()} To: {df_valid['saledate'].max()}\")\n",
    "print(\"\\nTest data date range:\")\n",
    "print(f\"From: {df_test['saledate'].min()} To: {df_test['saledate'].max()}\")\n",
    "\n",
    "# Visualizing the training data sales over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=df_train[\"saledate\"][:1000], # visualize the first 1000 values\n",
    "           y=df_train[\"SalePrice\"][:1000])\n",
    "ax.set_xlabel(\"Sale Date\")\n",
    "ax.set_ylabel(\"Sale Price ($)\")\n",
    "ax.set_title(\"Training Data: Sale Price vs Date\");\n",
    "\n",
    "# Sorting training data by date\n",
    "df_train.sort_values(by='saledate', ascending=True, inplace=True)\n",
    "print(\"Training data sorted by date\")\n",
    "df_train.head(10)\n",
    "\n",
    "# Sorting validation data by date\n",
    "df_valid.sort_values(by='saledate', ascending=True, inplace=True)\n",
    "print(\"Validation data sorted by date\")\n",
    "df_valid.head(10)\n",
    "\n",
    "# Sorting test data by date\n",
    "df_test.sort_values(by='saledate', ascending=True, inplace=True)\n",
    "print(\"Test data sorted by date\")\n",
    "df_test.head(10)\n",
    "\n",
    "# Function to add date features to any dataframe\n",
    "def add_datepart_features(df):\n",
    "    \"\"\"Add datetime parameters for saledate column\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"saleYear\"] = df_copy.saledate.dt.year\n",
    "    df_copy[\"saleMonth\"] = df_copy.saledate.dt.month\n",
    "    df_copy[\"saleDay\"] = df_copy.saledate.dt.day\n",
    "    df_copy[\"saleDayofweek\"] = df_copy.saledate.dt.dayofweek\n",
    "    df_copy[\"saleDayofyear\"] = df_copy.saledate.dt.dayofyear\n",
    "\n",
    "    # Drop original saledate column\n",
    "    df_copy.drop(\"saledate\", axis=1, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "# Adding date features to training data\n",
    "df_train_processed = add_datepart_features(df_train)\n",
    "print(\"Training data with date features:\")\n",
    "df_train_processed[[\"SalePrice\", \"saleYear\", \"saleMonth\", \"saleDay\", \"saleDayofweek\", \"saleDayofyear\"]].head()\n",
    "\n",
    "# Adding date features to validation data\n",
    "df_valid_processed = add_datepart_features(df_valid)\n",
    "print(\"Validation data with date features:\")\n",
    "df_valid_processed[[\"SalePrice\", \"saleYear\", \"saleMonth\", \"saleDay\", \"saleDayofweek\", \"saleDayofyear\"]].head()\n",
    "\n",
    "# Adding date features to test data\n",
    "df_test_processed = add_datepart_features(df_test)\n",
    "print(\"Test data with date features:\")\n",
    "df_test_processed[[\"saleYear\", \"saleMonth\", \"saleDay\", \"saleDayofweek\", \"saleDayofyear\"]].head()\n",
    "\n",
    "# Visualizing monthly sales patterns in training data\n",
    "df_train_processed.groupby([\"saleMonth\"])[\"SalePrice\"].median().plot()\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Median Sale Price ($)\")\n",
    "plt.title(\"Training Data: Median Sale Price by Month\");\n",
    "\n",
    "# Checking data info for training set\n",
    "print(\"Training data info:\")\n",
    "df_train_processed.info()\n",
    "\n",
    "# Checking missing values in training data\n",
    "print(\"Missing values in training data:\")\n",
    "print(df_train_processed.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# Function to analyze data types\n",
    "def analyze_data_types(df, dataset_name):\n",
    "    \"\"\"Analyze and print data types for object and numeric columns\"\"\"\n",
    "    print(f\"\\n= {dataset_name} Data Type Analysis =\")\n",
    "\n",
    "    # Object type columns\n",
    "    number_of_object_type_columns = 0\n",
    "    print(\"\\nObject type columns:\")\n",
    "    for label, content in df.items():\n",
    "        if pd.api.types.is_object_dtype(content):\n",
    "            column_datatype = df[label].dtype.name\n",
    "            example_value = content.sample(1).values\n",
    "            example_value_dtype = pd.api.types.infer_dtype(example_value)\n",
    "            print(f\"Column name: {label} | Column dtype: {column_datatype} | Example value: {example_value} | Example value dtype: {example_value_dtype}\")\n",
    "            number_of_object_type_columns += 1\n",
    "\n",
    "    print(f\"\\n[INFO] Total number of object type columns: {number_of_object_type_columns}\")\n",
    "\n",
    "    # Numeric type columns\n",
    "    print(\"\\nNumeric type columns:\")\n",
    "    for label, content in df.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            column_datatype = df[label].dtype.name\n",
    "            example_value = content.sample(1).values\n",
    "            example_value_dtype = pd.api.types.infer_dtype(example_value)\n",
    "            print(f\"Column name: {label} | Column dtype: {column_datatype} | Example value: {example_value} | Example value dtype: {example_value_dtype}\")\n",
    "\n",
    "# Analyze training data types\n",
    "analyze_data_types(df_train_processed, \"Training\")\n",
    "\n",
    "# Function to convert object columns to categories\n",
    "def convert_strings_to_categories(df):\n",
    "    \"\"\"Convert object type columns to category type\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for label, content in df_copy.items():\n",
    "        if pd.api.types.is_object_dtype(content):\n",
    "            df_copy[label] = content.astype('category')\n",
    "    return df_copy\n",
    "\n",
    "# Convert training data\n",
    "df_train_processed = convert_strings_to_categories(df_train_processed)\n",
    "print(\"Training data after converting strings to categories:\")\n",
    "df_train_processed.info()\n",
    "\n",
    "# Convert validation data\n",
    "df_valid_processed = convert_strings_to_categories(df_valid_processed)\n",
    "print(\"Validation data after converting strings to categories:\")\n",
    "df_valid_processed.info()\n",
    "\n",
    "# Convert test data\n",
    "df_test_processed = convert_strings_to_categories(df_test_processed)\n",
    "print(\"Test data after converting strings to categories:\")\n",
    "df_test_processed.info()\n",
    "\n",
    "# Function to handle missing values for numeric columns\n",
    "def fill_missing_numeric(df):\n",
    "    \"\"\"Fill missing values in numeric columns with median and create missing indicators\"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Check for which numeric columns have null values\n",
    "    print(\"Numeric columns with missing values:\")\n",
    "    for label, content in df_copy.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                print(f\"Column name: {label} | Has missing values: {True} | Count: {pd.isnull(content).sum()}\")\n",
    "            else:\n",
    "                print(f\"Column name: {label} | Has missing values: {False}\")\n",
    "\n",
    "    # Filling missing values with Median\n",
    "    for label, content in df_copy.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                df_copy[label+\"_is_missing\"] = pd.isnull(content).astype(int)\n",
    "                df_copy[label] = content.fillna(content.median())\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Fill missing values in training data\n",
    "df_train_processed = fill_missing_numeric(df_train_processed)\n",
    "print(\"\\nTraining data after filling missing numeric values\")\n",
    "\n",
    "# Fill missing values in validation data using training data medians\n",
    "def fill_missing_numeric_with_train_stats(df_valid, df_train):\n",
    "    \"\"\"Fill missing values in validation data using training data statistics\"\"\"\n",
    "    df_copy = df_valid.copy()\n",
    "\n",
    "    for label, content in df_copy.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                df_copy[label+\"_is_missing\"] = pd.isnull(content).astype(int)\n",
    "                # Use training data median for validation data\n",
    "                train_median = df_train[label].median()\n",
    "                df_copy[label] = content.fillna(train_median)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Fill missing values in validation data using training stats\n",
    "df_valid_processed = fill_missing_numeric_with_train_stats(df_valid_processed, df_train_processed)\n",
    "print(\"Validation data after filling missing numeric values with training stats\")\n",
    "\n",
    "# Fill missing values in test data using training data medians\n",
    "df_test_processed = fill_missing_numeric_with_train_stats(df_test_processed, df_train_processed)\n",
    "print(\"Test data after filling missing numeric values with training stats\")\n",
    "\n",
    "# Function to handle categorical columns and convert to numeric\n",
    "def handle_categorical_columns(df_train, df_valid, df_test):\n",
    "    \"\"\"\n",
    "    Process categorical columns by mapping validation and test datasets\n",
    "    to the same category codes as the training dataset.\n",
    "    Parameters:\n",
    "        df_train: DataFrame for training\n",
    "        df_valid: DataFrame for validation\n",
    "        df_test: DataFrame for testing\n",
    "    Returns:\n",
    "        Tuple: (Processed DataFrames, category mappings)\n",
    "    \"\"\"\n",
    "    column_to_category_dict = {}\n",
    "    df_train_copy = df_train.copy()\n",
    "    df_valid_copy = df_valid.copy()\n",
    "    df_test_copy = df_test.copy()\n",
    "\n",
    "    for label, content in df_train.items():\n",
    "        if not pd.api.types.is_numeric_dtype(content):\n",
    "            # Train Dataset: Convert to Categorical and Map Codes\n",
    "            train_categories = pd.Categorical(content)\n",
    "            df_train_copy[label] = train_categories.codes + 1\n",
    "            column_to_category_dict[label] = dict(\n",
    "                zip(range(1, len(train_categories.categories) + 1), train_categories.categories)\n",
    "            )\n",
    "\n",
    "            if label in df_valid.columns:\n",
    "                # Validation Dataset: Map Categories\n",
    "                valid_categories = pd.Categorical(df_valid[label], categories=train_categories.categories)\n",
    "                df_valid_copy[label] = valid_categories.codes + 1\n",
    "                # Handle unknown categories in validation\n",
    "                df_valid_copy[label] = df_valid_copy[label].fillna(0).astype(int)\n",
    "\n",
    "            if label in df_test.columns:\n",
    "                # Test Dataset: Map Categories\n",
    "                test_categories = pd.Categorical(df_test[label], categories=train_categories.categories)\n",
    "                df_test_copy[label] = test_categories.codes + 1\n",
    "                # Handle unknown categories in testing\n",
    "                df_test_copy[label] = df_test_copy[label].fillna(0).astype(int)\n",
    "\n",
    "    return (df_train_copy, df_valid_copy, df_test_copy), column_to_category_dict\n",
    "\n",
    "# Process all datasets with consistent categorical encoding\n",
    "processed_data, category_mappings = handle_categorical_columns(\n",
    "    df_train_processed, df_valid_processed, df_test_processed\n",
    ")\n",
    "# Corrected Code\n",
    "# Process all datasets and unpack them correctly\n",
    "(df_train_final,\n",
    " df_valid_final,\n",
    " df_test_final), category_mappings = handle_categorical_columns(\n",
    "    df_train_processed, df_valid_processed, df_test_processed\n",
    ")\n",
    "print(\"All datasets processed with consistent categorical encoding\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Final check for missing values:\")\n",
    "print(f\"Training data missing values: {df_train_final.isna().sum().sum()}\")\n",
    "print(f\"Validation data missing values: {df_valid_final.isna().sum().sum()}\")\n",
    "print(f\"Test data missing values: {df_test_final.isna().sum().sum()}\")\n",
    "\n",
    "# Importing machine learning libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Create features for all datasets\n",
    "X_train = df_train_final.drop(\"SalePrice\", axis=1)\n",
    "X_valid = df_valid_final.drop(\"SalePrice\", axis=1)\n",
    "X_test = df_test_final\n",
    "\n",
    "# Create log-transformed targets for RMSLE optimization\n",
    "y_train = np.log1p(df_train_final[\"SalePrice\"])  # log1p = log(1 + x) handles zeros safely\n",
    "y_valid = np.log1p(df_valid_final[\"SalePrice\"])\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training targets shape (log-transformed): {y_train.shape}\")\n",
    "print(f\"Validation features shape: {X_valid.shape}\")\n",
    "print(f\"Validation targets shape (log-transformed): {y_valid.shape}\")\n",
    "\n",
    "# Align columns of validation and test sets with training set\n",
    "missing_cols_valid = set(X_train.columns) - set(X_valid.columns)\n",
    "for c in missing_cols_valid:\n",
    "    X_valid[c] = 0\n",
    "missing_cols_test = set(X_train.columns) - set(X_test.columns)\n",
    "for c in missing_cols_test:\n",
    "    X_test[c] = 0\n",
    "\n",
    "# Ensure the order of columns is the same\n",
    "X_valid = X_valid[X_train.columns]\n",
    "X_test = X_test[X_train.columns]\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Function to calculate RMSLE optimized for log-transformed targets\n",
    "def rmsle_optimized(y_true_log, y_pred_log):\n",
    "    \"\"\"Calculate RMSE on log-transformed values (equivalent to RMSLE on original scale)\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true_log, y_pred_log))\n",
    "\n",
    "# Train initial model on log-transformed targets\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "print(\"Training initial model on log-transformed targets...\")\n",
    "model.fit(X=X_train, y=y_train)\n",
    "print(\"Initial model training completed!\")\n",
    "\n",
    "# Making predictions on training data (in log space)\n",
    "y_train_pred_log = model.predict(X_train)\n",
    "train_rmsle = rmsle_optimized(y_train, y_train_pred_log)\n",
    "\n",
    "# Convert back to original scale for MAE calculation\n",
    "y_train_pred_original = np.expm1(y_train_pred_log)\n",
    "y_train_original = np.expm1(y_train)\n",
    "train_mae = mean_absolute_error(y_train_original, y_train_pred_original)\n",
    "\n",
    "print(f\"Training RMSLE: {train_rmsle:.4f}\")\n",
    "print(f\"Training MAE: ${train_mae:,.2f}\")\n",
    "\n",
    "# Making predictions on validation data for initial evaluation\n",
    "y_valid_pred_log = model.predict(X_valid)\n",
    "valid_rmsle = rmsle_optimized(y_valid, y_valid_pred_log)\n",
    "\n",
    "# Convert back to original scale for MAE calculation\n",
    "y_valid_pred_original = np.expm1(y_valid_pred_log)\n",
    "y_valid_original = np.expm1(y_valid)\n",
    "valid_mae = mean_absolute_error(y_valid_original, y_valid_pred_original)\n",
    "\n",
    "print(f\"Validation RMSLE: {valid_rmsle:.4f}\")\n",
    "print(f\"Validation MAE: ${valid_mae:,.2f}\")\n",
    "\n",
    "# Define hyperparameter grid for optimization\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create TimeSeriesSplit for proper time-series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(\"Created TimeSeriesSplit with 3 splits for proper temporal validation\")\n",
    "\n",
    "# Create RandomizedSearchCV with time-series aware cross-validation\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=25,\n",
    "    cv=tscv,  # Use TimeSeriesSplit instead of default CV\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'  # MSE on log-transformed targets = RMSLE optimization\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning with proper time-series validation\n",
    "print(\"Starting hyperparameter tuning with TimeSeriesSplit (no data leakage)...\")\n",
    "rf_random.fit(X_train, y_train)\n",
    "print(\"Hyperparameter tuning completed!\")\n",
    "\n",
    "# Get the best model from hyperparameter tuning\n",
    "best_model = rf_random.best_estimator_\n",
    "print(f\"Best parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best cross-validation RMSE (log scale): {np.sqrt(-rf_random.best_score_):.4f}\")\n",
    "\n",
    "# Evaluate best model on validation data\n",
    "y_valid_pred_best_log = best_model.predict(X_valid)\n",
    "valid_rmsle_best = rmsle_optimized(y_valid, y_valid_pred_best_log)\n",
    "\n",
    "# Convert to original scale for interpretability\n",
    "y_valid_pred_best_original = np.expm1(y_valid_pred_best_log)\n",
    "valid_mae_best = mean_absolute_error(y_valid_original, y_valid_pred_best_original)\n",
    "\n",
    "print(f\"Best Model Validation RMSLE: {valid_rmsle_best:.4f}\")\n",
    "print(f\"Best Model Validation MAE: ${valid_mae_best:,.2f}\")\n",
    "\n",
    "# Combine training and validation data for final model training\n",
    "print(\"Combining training and validation data for final model...\")\n",
    "X_combined = pd.concat([X_train, X_valid], axis=0, ignore_index=True)\n",
    "y_combined = pd.concat([\n",
    "    pd.Series(y_train, name='SalePrice_log'),\n",
    "    pd.Series(y_valid, name='SalePrice_log')\n",
    "], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {X_combined.shape}\")\n",
    "print(f\"Combined targets shape: {y_combined.shape}\")\n",
    "\n",
    "# Create final model with best hyperparameters\n",
    "final_model = RandomForestRegressor(\n",
    "    **rf_random.best_params_,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train final model on all available data\n",
    "print(\"Training final model on combined training + validation data...\")\n",
    "final_model.fit(X_combined, y_combined)\n",
    "print(\"Final model training completed!\")\n",
    "\n",
    "# Estimate final model performance using cross-validation on combined data\n",
    "print(\"Estimating final model performance with cross-validation...\")\n",
    "cv_scores = cross_val_score(\n",
    "    final_model,\n",
    "    X_combined,\n",
    "    y_combined,\n",
    "    cv=TimeSeriesSplit(n_splits=5),\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "final_cv_std = np.sqrt(cv_scores.std())\n",
    "print(f\"Final Model CV RMSLE: {final_cv_rmse:.4f} (+/- {final_cv_std:.4f})\")\n",
    "\n",
    "# Make final predictions on test set\n",
    "print(\"Making final predictions on test set...\")\n",
    "y_test_pred_log = final_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original scale for submission\n",
    "y_test_pred_final = np.expm1(y_test_pred_log)\n",
    "\n",
    "print(f\"Test predictions completed!\")\n",
    "print(f\"Test predictions shape: {y_test_pred_final.shape}\")\n",
    "print(f\"Sample test predictions: {y_test_pred_final[:10]}\")\n",
    "\n",
    "# Create optimized submission file\n",
    "submission_final = pd.DataFrame({\n",
    "    'SalesID': df_test['SalesID'],\n",
    "    'SalePrice': y_test_pred_final\n",
    "})\n",
    "\n",
    "submission_final.to_csv('bulldozer_price_predictions_optimized.csv', index=False)\n",
    "print(\"Optimized submission file created: bulldozer_price_predictions_optimized.csv\")\n",
    "\n",
    "# Feature importance analysis using final model\n",
    "feature_importance_final = pd.DataFrame({\n",
    "    'feature': X_combined.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features from final optimized model:\")\n",
    "print(feature_importance_final.head(20))\n",
    "\n",
    "# Plot feature importance for final model\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_final.head(20)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Feature Importances - Final Optimized Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== OPTIMIZATION SUMMARY ===\")\n",
    "print(\"✅ Log transformation applied for RMSLE optimization\")\n",
    "print(\"✅ TimeSeriesSplit used to prevent data leakage\")\n",
    "print(\"✅ Final model trained on combined training + validation data\")\n",
    "print(\"✅ All predictions properly converted from log space to original scale\")\n",
    "print(\"✅ Model optimized for competition metric (RMSLE)\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "41d460ba11b14599"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
